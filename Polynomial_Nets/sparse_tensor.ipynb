{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from torchsummary import summary\n",
    "\n",
    "from random import randint\n",
    "\n",
    "from polynomial_nets import CP_L3, CP_L3_sparse\n",
    "\n",
    "from poly_VAE import Flatten, UnFlatten, VAE_CP_L3, VAE_CP_L3_sparse, VAE_CP_L3_sparse_LU, loss_fn\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download training dataset\n",
    "dataset = MNIST(root='data/', download=True)\n",
    "# MNIST dataset (images and labels)\n",
    "dataset = MNIST(root='data/', train=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds = random_split(dataset, [50000, 10000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_ds, BATCH_SIZE, shuffle=True)\n",
    "#train_loader = DeviceDataLoader(train_loader, device)\n",
    "#val_loader = DataLoader(val_ds, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "for images,_ in train_loader:\n",
    "    print(images[0].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tensor = torch.randn((1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size = 3\n",
    "stride = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.tensor([[1,2,3], [4,5,6], [7,8,9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_reshape = test.reshape((9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_range = torch.arange(0,27).reshape((3,3,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://leimao.github.io/blog/Convolution-Transposed-Convolution-As-Matrix-Multiplication/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d(X, K):\n",
    "\n",
    "    # Convolution in deep learning is a misnomer.\n",
    "    # In fact, it is cross-correlation.\n",
    "    # https://d2l.ai/chapter_convolutional-neural-networks/conv-layer.html\n",
    "    # This is equivalent as Conv2D that that input_channel == output_channel == 1 and stride == 1.\n",
    "\n",
    "    assert X.dim() == 2 and K.dim() == 2\n",
    "\n",
    "    h, w = K.shape\n",
    "    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i, j] = (X[i:i + h, j:j + w] * K).sum()\n",
    "\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_as_matrix_mul(X, K):\n",
    "\n",
    "    # Assuming no channels and stride == 1.\n",
    "    # Convert the kernel matrix to sparse matrix (dense matrix with lots of zeros in fact).\n",
    "    # This is a little bit brain-twisting.\n",
    "\n",
    "    h_K, w_K = K.shape\n",
    "    h_X, w_X = X.shape\n",
    "\n",
    "    h_Y, w_Y = h_X - h_K + 1, w_X - w_K + 1\n",
    "\n",
    "    W = get_sparse_kernel_matrix(K=K, h_X=h_X, w_X=w_X)\n",
    "\n",
    "    Y = torch.matmul(W, X.reshape(-1)).reshape(h_Y, w_Y)\n",
    "\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_transposed_2d_as_matrix_mul(X, K):\n",
    "\n",
    "    # Assuming no channels and stride == 1.\n",
    "    # Convert the kernel matrix to sparse matrix (dense matrix with lots of zeros in fact).\n",
    "    # This is a little bit brain-twisting.\n",
    "\n",
    "    h_K, w_K = K.shape\n",
    "    h_X, w_X = X.shape\n",
    "\n",
    "    h_Y, w_Y = h_X + h_K - 1, w_X + w_K - 1\n",
    "\n",
    "    # It's like the kernel were applied on the output tensor.\n",
    "    W = get_sparse_kernel_matrix(K=K, h_X=h_Y, w_X=w_Y)\n",
    "\n",
    "    # Weight matrix tranposed.\n",
    "    Y = torch.matmul(W.T, X.reshape(-1)).reshape(h_Y, w_Y)\n",
    "\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sparse_kernel_matrix(K, h_X, w_X):\n",
    "\n",
    "    # Assuming no channels and stride == 1.\n",
    "    # Convert the kernel matrix to sparse matrix (dense matrix with lots of zeros in fact).\n",
    "    # This is a little bit brain-twisting.\n",
    "\n",
    "    h_K, w_K = K.shape\n",
    "\n",
    "    h_Y, w_Y = h_X - h_K + 1, w_X - w_K + 1\n",
    "\n",
    "    W = torch.zeros((h_Y * w_Y, h_X * w_X))\n",
    "    for i in range(h_Y):\n",
    "        for j in range(w_Y):\n",
    "            for ii in range(h_K):\n",
    "                for jj in range(w_K):\n",
    "                    W[i * w_Y + j, i * w_X + j + ii * w_X + jj] = K[ii, jj]\n",
    "\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.arange(28*28).reshape(28, 28).float()\n",
    "K = torch.arange(9).reshape(3, 3).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_K, w_K = K.shape \n",
    "h_X, w_X = X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = get_sparse_kernel_matrix(K, h_X, w_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = torch.matmul(W, X.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1 = torch.matmul(W.T, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_r = X.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_shape = W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_index = torch.tensor([[0, 1], [1, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = torch.zeros([5,3])\n",
    "indices = torch.LongTensor([[0,1], [1, 2], [2, 2], [3, 0], [4, 1]])\n",
    "value = torch.ones(indices.shape[0])\n",
    "target.index_put_(tuple(indices.t()), value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 3, 4],\n",
       "        [1, 2, 2, 0, 1]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_matrix = torch.zeros(W_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = torch.nonzero(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = torch.randn(W_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#res = matrix[indices[:,[0]], indices[:,[1]]]\n",
    "#res = matrix[test_index]\n",
    "#res = W[indices[:,[0]], indices[:,[1]]]\n",
    "res1 = W[indices[:,0], indices[:,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 2.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 8., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 7., 8., 0.],\n",
       "        [0., 0., 0.,  ..., 6., 7., 8.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_matrix.index_put_(tuple(indices.t()), res1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(W -  zero_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_width = 4\n",
    "kernel_height = kernel_width\n",
    "image_width = 16\n",
    "image_height = image_width\n",
    "stride = 2\n",
    "width_steps = int((image_width - kernel_width)/stride)\n",
    "height_steps = int((image_height - kernel_height)/stride)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = torch.arange(0, kernel_width)\n",
    "base_new = base.repeat(kernel_height)\n",
    "addition = torch.arange(0, kernel_width) * image_width\n",
    "addition_new  = addition.repeat_interleave(kernel_height)\n",
    "index_1 = addition_new + base_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_width = index_1.repeat(width_steps + 1)\n",
    "addition_width = 2 * torch.arange(0, width_steps + 1)\n",
    "addition_width = addition_width.repeat_interleave(kernel_height * kernel_width)\n",
    "index_row = index_width + addition_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_column = index_row.repeat(height_steps + 1)\n",
    "addition_height = 2 * image_width * torch.arange(0, height_steps + 1)\n",
    "addition_height_rep = addition_height.repeat_interleave(kernel_height * kernel_width * (height_steps + 1))\n",
    "index_final = index_column + addition_height_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack = torch.arange(0, (height_steps + 1) * (width_steps + 1)).repeat_interleave(kernel_height * kernel_width)\n",
    "indices = torch.stack((stack, index_final), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = torch.randn(indices.shape[0])\n",
    "values1 = torch.nn.Linear(indices.shape[0], 1).weight.to(torch.float64)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Index put requires the source and destination dtypes match, got Double for the destination and Float for the source.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros([W_out, W_in], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat64)\n\u001b[1;32m     11\u001b[0m values_m \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones_like(values)\n\u001b[0;32m---> 12\u001b[0m mask \u001b[39m=\u001b[39m mask\u001b[39m.\u001b[39;49mindex_put_(\u001b[39mtuple\u001b[39;49m(indices\u001b[39m.\u001b[39;49mt()), values_m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Index put requires the source and destination dtypes match, got Double for the destination and Float for the source."
     ]
    }
   ],
   "source": [
    "W_in = image_height * image_width\n",
    "W_out = (height_steps + 1) * (width_steps + 1)\n",
    "W = torch.zeros([W_out, W_in], dtype=torch.float64)\n",
    "W = W.index_put_(tuple(indices.t()), values1)\n",
    "\n",
    "W = torch.zeros([W_out, W_in], dtype=torch.float32)\n",
    "values = torch.nn.Linear(indices.shape[0], 1).weight.to(torch.float32)[0]\n",
    "W = W.index_put_(tuple(indices.t()), values)\n",
    "\n",
    "mask = torch.zeros([W_out, W_in], dtype=torch.float64)\n",
    "values_m = torch.ones_like(values)\n",
    "mask = mask.index_put_(tuple(indices.t()), values_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.dtype\n",
    "values1.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_height * kernel_width * (height_steps + 1) * (width_steps + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_matrix_k4s2(image_size):\n",
    "    \n",
    "    kernel_width = 4\n",
    "    kernel_height = kernel_width\n",
    "    image_width = image_size\n",
    "    image_height = image_width\n",
    "    stride = 2\n",
    "    width_steps = int((image_width - kernel_width)/stride)\n",
    "    height_steps = int((image_height - kernel_height)/stride)\n",
    "\n",
    "    base = torch.arange(0, kernel_width)\n",
    "    base_new = base.repeat(kernel_height)\n",
    "    addition = torch.arange(0, kernel_width) * image_width\n",
    "    addition_new  = addition.repeat_interleave(kernel_height)\n",
    "    index_1 = addition_new + base_new\n",
    "\n",
    "    index_width = index_1.repeat(width_steps + 1)\n",
    "    addition_width = stride * torch.arange(0, width_steps + 1)\n",
    "    addition_width = addition_width.repeat_interleave(kernel_height * kernel_width)\n",
    "    index_row = index_width + addition_width\n",
    "\n",
    "    index_column = index_row.repeat(height_steps + 1)\n",
    "    addition_height = stride * image_width * torch.arange(0, height_steps + 1)\n",
    "    addition_height_rep = addition_height.repeat_interleave(kernel_height * kernel_width * (height_steps + 1))\n",
    "    index_final = index_column + addition_height_rep\n",
    "\n",
    "    stack = torch.arange(0, (height_steps + 1) * (width_steps + 1)).repeat_interleave(kernel_height * kernel_width)\n",
    "    indices = torch.stack((stack, index_final), dim=1)\n",
    "\n",
    "    W_in = image_height * image_width\n",
    "    W_out = (height_steps + 1) * (width_steps + 1)\n",
    "    W = torch.zeros([W_out, W_in], dtype=torch.float64)\n",
    "    values = torch.nn.Linear(indices.shape[0], 1).weight.to(torch.float64)[0]\n",
    "    W = W.index_put_(tuple(indices.t()), values)\n",
    "\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = weight_matrix_k4s2(32)\n",
    "W2 = weight_matrix_k4s2(32)\n",
    "W3 = torch.vstack((W1, W2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_tup = [W1]*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0013, -0.0071,  0.0122,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.0031,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ..., -0.0159,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ..., -0.0165, -0.0135, -0.0124]],\n",
       "       dtype=torch.float64, grad_fn=<IndexPutBackward0>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[weight_matrix_k4s2(d)[0]]*6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = weight_matrix_k4s2(32).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "225"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(32.)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sqrt(torch.tensor(32*32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "542851ebc7f1b97dab2659843c1270f7c570d916e83d4380bfc02112104e2680"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
